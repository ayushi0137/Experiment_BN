# Experiment_BN

In this notebook, we will discuss how the batch normalization helps in building an efficient model.  Batch normalization is a feature that we add between the layers of the neural network and it continuously takes the output from the previous layer and normalizes it before sending it to the next layer. This has the effect of stabilizing the neural network. Batch normalization is also used to maintain the distribution of the data.
